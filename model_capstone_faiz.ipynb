{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Data processing**"
      ],
      "metadata": {
        "id": "43uxve_tQ4Sw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PsY11taO8Z6y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "data = pd.read_csv('filtered_kz.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.DataFrame()\n",
        "remaining_data = data.copy()\n",
        "\n",
        "# Add at least one interaction per user to the training set\n",
        "for user in data['user_id'].unique():\n",
        "    user_data = remaining_data[remaining_data['user_id'] == user]\n",
        "    train_data = pd.concat([train_data, user_data.iloc[:1]])\n",
        "    remaining_data = remaining_data.drop(user_data.index[:1])\n",
        "\n",
        "# Add at least one interaction per product to the training set\n",
        "for product in data['product_id'].unique():\n",
        "    product_data = remaining_data[remaining_data['product_id'] == product]\n",
        "    if not product_data.empty:\n",
        "        train_data = pd.concat([train_data, product_data.iloc[:1]])\n",
        "        remaining_data = remaining_data.drop(product_data.index[:1])\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split remaining data into validation and test sets\n",
        "val_data, test_data = train_test_split(remaining_data, test_size=0.5, random_state=42)\n",
        "\n",
        "# Add the remaining data to training set to balance sizes if needed\n",
        "train_data = pd.concat([train_data, remaining_data])\n"
      ],
      "metadata": {
        "id": "EMzYNEqr0L64"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Train size: {train_data.shape}, Validation size: {val_data.shape}, Test size: {test_data.shape}\")\n",
        "print(f\"Unique users in Train: {train_data['user_id'].nunique()}, Validation: {val_data['user_id'].nunique()}, Test: {test_data['user_id'].nunique()}\")\n",
        "print(f\"Unique products in Train: {train_data['product_id'].nunique()}, Validation: {val_data['product_id'].nunique()}, Test: {test_data['product_id'].nunique()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-d3BpE01rrT",
        "outputId": "1cacbccc-3b8a-46e3-9b55-99e340783328"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: (37573, 8), Validation size: (10011, 8), Test size: (10012, 8)\n",
            "Unique users in Train: 15135, Validation: 2603, Test: 2537\n",
            "Unique products in Train: 2827, Validation: 1517, Test: 1495\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Initialize Parameter**"
      ],
      "metadata": {
        "id": "-IgnzUep0FYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Combine all data to extract unique users and products\n",
        "all_data = pd.concat([train_data, val_data, test_data])\n",
        "\n",
        "# Create mappings for user_id and product_id to indices\n",
        "unique_users = all_data['user_id'].unique()\n",
        "unique_products = all_data['product_id'].unique()\n",
        "\n",
        "user_to_index = {user: idx for idx, user in enumerate(unique_users)}\n",
        "product_to_index = {product: idx for idx, product in enumerate(unique_products)}\n",
        "\n",
        "# Function to create R matrix\n",
        "def create_r_matrix(data, num_products, num_users):\n",
        "    R = np.zeros((num_products, num_users), dtype=int)\n",
        "    for _, row in data.iterrows():\n",
        "        user_idx = user_to_index[row['user_id']]\n",
        "        product_idx = product_to_index[row['product_id']]\n",
        "        R[product_idx, user_idx] = 1\n",
        "    return R\n",
        "\n",
        "# Number of products and users\n",
        "num_products = len(unique_products)\n",
        "num_users = len(unique_users)\n",
        "\n",
        "# Create R matrices for train, validation, and test sets\n",
        "R_train = create_r_matrix(train_data, num_products, num_users)\n",
        "R_val = create_r_matrix(val_data, num_products, num_users)\n",
        "R_test = create_r_matrix(test_data, num_products, num_users)\n",
        "\n",
        "# Check dimensions\n",
        "print(\"R_train shape:\", R_train.shape)\n",
        "print(\"R_val shape:\", R_val.shape)\n",
        "print(\"R_test shape:\", R_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjFHHVMBRMLq",
        "outputId": "52e9d5da-754b-4a92-de36-5a787bbca573"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R_train shape: (2827, 15135)\n",
            "R_val shape: (2827, 15135)\n",
            "R_test shape: (2827, 15135)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#find Y\n",
        "Y_train = R_train\n",
        "Y_val = R_val\n",
        "Y_test = R_test"
      ],
      "metadata": {
        "id": "3fBOt0m5ST3I"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#find X (optional)\n",
        "#explicit\n",
        "\n",
        "item_features = data[['product_id', 'category_code']].drop_duplicates()\n",
        "# One-hot encode category codes\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "explicit_features = encoder.fit_transform(item_features[['category_code']])\n",
        "# Convert to NumPy array\n",
        "explicit_features = np.array(explicit_features)\n",
        "# Convert explicit features to a TensorFlow constant\n",
        "explicit_features = tf.constant(explicit_features, dtype=tf.float64)\n",
        "\n",
        "#laten\n",
        "\n",
        "num_latent_features = 10\n",
        "# Randomly initialize latent features\n",
        "latent_features = tf.Variable(\n",
        "    tf.random.normal((explicit_features.shape[0], num_latent_features), dtype=tf.float64),\n",
        "    name=\"latent_features\"\n",
        ")\n",
        "\n",
        "# Combine explicit and latent features\n",
        "X = tf.concat([explicit_features, latent_features], axis=1)\n",
        "\n",
        "# Check the shape\n",
        "print(\"Combined product features (X) shape:\", X.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOExJtffSY8b",
        "outputId": "1ff50387-acad-43b3-fcdb-552269cb94dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined product features (X) shape: (2827, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Useful Values\n",
        "num_products, num_users = Y_train.shape\n",
        "num_features = 15\n",
        "# Set Initial Parameters (W, X), use tf.Variable to track these variables\n",
        "tf.random.set_seed(1234) # for consistent results\n",
        "W = tf.Variable(tf.random.normal((num_users,  num_features),dtype=tf.float64),  name='W')\n",
        "X = tf.Variable(tf.random.normal((num_products,  num_features),dtype=tf.float64),  name='X')\n",
        "b = tf.Variable(tf.random.normal((1,          num_users),   dtype=tf.float64),  name='b')\n",
        "\n",
        "# Instantiate an optimizer.\n",
        "optimizer = keras.optimizers.Adam(learning_rate=1e-1)\n"
      ],
      "metadata": {
        "id": "X6GSGWbvgLxw"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model**"
      ],
      "metadata": {
        "id": "ysrikFm4eyhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_cf_loss(X, W, b, Y, R, lambda_):\n",
        "    \"\"\"\n",
        "    Returns the cost for the content-based filtering\n",
        "    Vectorized for speed. Uses tensorflow operations to be compatible with custom training loop.\n",
        "    Args:\n",
        "      X (ndarray (num_movies,num_features)): matrix of item features\n",
        "      W (ndarray (num_users,num_features)) : matrix of user parameters\n",
        "      b (ndarray (1, num_users)            : vector of user parameters\n",
        "      Y (ndarray (num_movies,num_users)    : matrix of user ratings of movies\n",
        "      R (ndarray (num_movies,num_users)    : matrix, where R(i, j) = 1 if the i-th movies was rated by the j-th user\n",
        "      lambda_ (float): regularization parameter\n",
        "    Returns:\n",
        "      J (float) : Cost\n",
        "    \"\"\"\n",
        "    z= tf.sigmoid(tf.matmul(X, tf.transpose(W)) + b)\n",
        "    # Cast R to tf.float64 before reduce_sum to match the dtype of the numerator\n",
        "    j = -tf.reduce_sum(R * (Y * tf.math.log(z) + (1 - Y) * tf.math.log(1 - z))) / tf.reduce_sum(tf.cast(R, dtype=tf.float64))\n",
        "    j += (lambda_ / 2) * (tf.reduce_sum(tf.square(X)) + tf.reduce_sum(tf.square(W)))\n",
        "    return j"
      ],
      "metadata": {
        "id": "zbIrLYkyhSeR"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iterations = 80\n",
        "lambda_ = 1\n",
        "for iter in range(iterations):\n",
        "    with tf.GradientTape() as tape:\n",
        "        cost_value = binary_cf_loss(X, W, b, Y_train, R_train, lambda_)\n",
        "\n",
        "    grads = tape.gradient( cost_value, [X,W,b] )\n",
        "    optimizer.apply_gradients( zip(grads, [X,W,b]) )\n",
        "\n",
        "    # Log periodically.\n",
        "    if iter % 20 == 0:\n",
        "        print(f\"Training loss at iteration {iter}: {cost_value:0.1f}\")"
      ],
      "metadata": {
        "id": "NwlCJEzohX_X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ce2751d-4c31-4ae4-ef14-6bd8809352d3"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss at iteration 0: 134434.5\n",
            "Training loss at iteration 20: 5255.5\n",
            "Training loss at iteration 40: 866.5\n",
            "Training loss at iteration 60: 111.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#val loss\n",
        "val_loss = binary_cf_loss(X, W, b, Y_val, R_val, lambda_)\n",
        "print(f\"Validation loss: {val_loss:0.4f}\")\n",
        "#test loss\n",
        "test_loss = binary_cf_loss(X, W, b, Y_test, R_test, lambda_)\n",
        "print(f\"Test loss: {test_loss:0.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uvQcYxBbifq",
        "outputId": "1e2c6363-27b7-496c-f0f8-979745846ba2"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 14.5480\n",
            "Test loss: 14.5478\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Predik**"
      ],
      "metadata": {
        "id": "lu5y3pyBjPlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_purchase(X, W, b):\n",
        "    return tf.sigmoid(tf.matmul(X, tf.transpose(W)) + b)\n",
        "# Predict purchase\n",
        "Y_hat = predict_purchase(X, W, b).numpy()\n",
        "\n",
        "def recommend_products(Y_hat, R, user_index, top_n=5):\n",
        "    # Get predicted ratings for the user\n",
        "    user_purchase = Y_hat[:, user_index]\n",
        "\n",
        "    # Exclude already purchased products\n",
        "    user_purchase[R[:, user_index] > 0] = -np.inf\n",
        "\n",
        "    # Get top-N product indices\n",
        "    top_products = np.argsort(user_purchase)[-top_n:][::-1]\n",
        "    return top_products\n",
        "\n",
        "# Example: Recommend top 5 products for user 0\n",
        "user_index = 0\n",
        "R=R_test+R_train+R_val\n",
        "top_products = recommend_products(Y_hat, R, user_index, top_n=5)\n",
        "print(\"Top recommended product indices:\", top_products)"
      ],
      "metadata": {
        "id": "CTE_Tdh2jKe6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b9663b8-5649-43fb-c657-b038458540fe"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top recommended product indices: [1409 2277 1889 1249 2435]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define input layers with the shapes of X, W, b\n",
        "input_X = keras.Input(shape=X.shape[1:], dtype=X.dtype)\n",
        "input_W = keras.Input(shape=W.shape[1:], dtype=W.dtype)\n",
        "input_b = keras.Input(shape=b.shape[1:], dtype=b.dtype)\n",
        "\n",
        "# Wrap the predict_purchase logic within a custom Keras Layer\n",
        "class PurchasePredictionLayer(keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(PurchasePredictionLayer, self).__init__()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        X, W, b = inputs\n",
        "        return tf.sigmoid(tf.matmul(X, tf.transpose(W)) + b)\n",
        "\n",
        "# Create an instance of the custom layer\n",
        "purchase_prediction_layer = PurchasePredictionLayer()\n",
        "\n",
        "# Create the model using the custom layer\n",
        "model = keras.Model(inputs=[input_X, input_W, input_b],\n",
        "                    outputs=purchase_prediction_layer([input_X, input_W, input_b]))\n",
        "\n",
        "# Instead of setting weights on input layers,\n",
        "# pass the initial values as input to the model during prediction\n",
        "#  Example:\n",
        "#  predictions = model.predict([X.numpy(), W.numpy(), b.numpy()])\n",
        "\n",
        "# Save the model\n",
        "model.save('collaborative_filtering_purchase.keras')"
      ],
      "metadata": {
        "id": "Tr9PBkF2tE3y"
      },
      "execution_count": 75,
      "outputs": []
    }
  ]
}